# Dataset Zoo

Datasets preparing was used from [Towards-Realtime-MOT](https://github.com/Zhongdao/Towards-Realtime-MOT/blob/master/DATASET_ZOO.md)

## Data Format

The root folder of datasets will have the following structure:

```text
.
└─datasets
  ├─Caltech
  ├─Cityscapes
  ├─CUHKSYSU
  ├─ETHZ
  ├─MOT16
  ├─MOT17
  └─PRW
```

Every image has a corresponding annotation text. Given an image path,
the annotation text path can be generated by replacing the string `images` with `labels_with_ids` and replacing `.jpg` with `.txt`.

In the annotation text, each line is describing a bounding box and has the following format:

```text
[class] [identity] [x_center] [y_center] [width] [height]
```

The field `[class]` should be `0`. Only single-class multi-object tracking is supported.

The field `[identity]` is an integer from `0` to `num_identities - 1`, or `-1` if this box has no identity annotation.

- Note that the values of `[x_center] [y_center] [width] [height]` are normalized by the width/height of the image, so they are floating point numbers ranging from 0 to 1.

## Download

### Caltech Pedestrian

Download all archives `set**.tar` files from [this page](https://drive.google.com/drive/folders/1IBlcJP8YsCaT81LwQ2YwQJac8bf1q8xF?usp=sharing) and extract to `Caltech/data`.

Download [annotations](https://drive.google.com/file/d/1h8vxl_6tgi9QVYoer9XcY9YwNB32TE5k/view?usp=sharing) and unzip to `Caltech/data/labels_with_ids`.

Download [this tool](https://github.com/mitmul/caltech-pedestrian-dataset-converter) to convert the original data format to images.
Move `scripts` folder of tool to `Caltech` folder and use command:

```bash
python scripts/convert_seqs.py
```

The structure of the dataset after completing all steps will be the following:

```text
.
└─Caltech
  └─data
    ├─images
    │ └─***
    └─labels_with_ids
      └─***
```

Note: *** - it is a data (images or annotations)

### CityPersons

Google Drive:
[[0]](https://drive.google.com/file/d/1DgLHqEkQUOj63mCrS_0UGFEM9BG8sIZs/view?usp=sharing)
[[1]](https://drive.google.com/file/d/1BH9Xz59UImIGUdYwUR-cnP1g7Ton_LcZ/view?usp=sharing)
[[2]](https://drive.google.com/file/d/1q_OltirP68YFvRWgYkBHLEFSUayjkKYE/view?usp=sharing)
[[3]](https://drive.google.com/file/d/1VSL0SFoQxPXnIdBamOZJzHrHJ1N2gsTW/view?usp=sharing)

Download `.zip` archives from links and use the following commands.

```bash
zip --FF Citypersons --out c.zip
unzip c.zip
mv Citypersons Cityscapes
```

The structure of the dataset after completing all steps will be the following:

```text
.
└─Cityscapes
  ├─images
  │ ├─train
  │ └─val
  └─labels_with_ids
    ├─train
    └─val
```

### CUHK-SYSU

Google Drive:
[[0]](https://drive.google.com/file/d/1D7VL43kIV9uJrdSCYl53j89RE2K-IoQA/view?usp=sharing)

Original dataset webpage: [CUHK-SYSU Person Search Dataset](http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html)

Download dataset, unzip and use command below.

```bash
mv CUHK-SYSU CUHKSYSU
```

The structure of the dataset will be the following:

```text
.
└─CUHKSYSU
  ├─images
  │ └─***
  └─labels_with_ids
    └─***
```

Note: *** - it is a data (images or annotations)

### PRW

Google Drive:
[[0]](https://drive.google.com/file/d/116_mIdjgB-WJXGe8RYJDWxlFnc_4sqS8/view?usp=sharing)

Download dataset and unzip. The structure of the dataset will be the following:

```text
.
└─PRW
  ├─images
  │ └─***
  └─labels_with_ids
    └─***
```

Note: *** - it is a data (images or annotations)

### ETHZ (overlapping with MOT-16 removed)

Google Drive:
[[0]](https://drive.google.com/file/d/19QyGOCqn8K_rc9TXJ8UwLSxCx17e0GoY/view?usp=sharing)

Download dataset and unzip. The structure of the dataset will be the following:

```text
.
└─ETHZ
  ├─eth01
  │ ├─images
  │ │ └─***
  │ └─labels_with_ids
  │   └─***
  ├─eth02
  ├─eth03
  ├─eth05
  └─eth07
```

Note: *** - it is a data (images or annotations). Same structure to every 'eth*' folder.

### MOT-17

Official link:
[[0]](https://motchallenge.net/data/MOT17.zip)

Original dataset webpage: [MOT-17](https://motchallenge.net/data/MOT17/)

After downloading, unzip and use `prepare_mot17.py` script from the:

```bash
python data/prepare_mot17.py --seq_root /path/to/MOT17/train
```

The structure of the dataset after completing all steps will be the following:

```text
.
└─MOT17
  ├─images
  │ └─train
  └─labels_with_ids
    └─train
```

### MOT-16 (for evaluation)

Google Drive:
[[0]](https://drive.google.com/file/d/1254q3ruzBzgn4LUejDVsCtT05SIEieQg/view?usp=sharing)

Original dataset webpage: [MOT-16](https://motchallenge.net/data/MOT16/)

Download link: [MOT-16.zip](https://motchallenge.net/data/MOT16.zip)

> The section "Download" in the bottom of the web-page. Link: "Get all data".

Download dataset and unzip. The structure of the dataset will be the following:

```text
.
└─MOT16
  └─train
```

# Data config

Download [schemas](https://github.com/Zhongdao/Towards-Realtime-MOT/tree/master/data) of the training data with relative paths for every image, divided into train/val parts and move into `data` folder.

```text
.
└── data
    ├─ caltech.10k.val
    ├─ caltech.train
    ├─ caltech.val
    ├─ citypersons.train
    ├─ citypersons.val
    ├─ cuhksysu.train
    ├─ cuhksysu.val
    ├─ eth.train
    ├─ mot17.train
    ├─ prw.train
    └─ prw.val
```

# Citation

Caltech:

```text
@inproceedings{ dollarCVPR09peds,
       author = "P. Doll\'ar and C. Wojek and B. Schiele and  P. Perona",
       title = "Pedestrian Detection: A Benchmark",
       booktitle = "CVPR",
       month = "June",
       year = "2009",
       city = "Miami",
}
```

Citypersons:

```text
@INPROCEEDINGS{Shanshan2017CVPR,
  author = {Shanshan Zhang and Rodrigo Benenson and Bernt Schiele},
  title = {CityPersons: A Diverse Dataset for Pedestrian Detection},
  booktitle = {CVPR},
  year = {2017}
 }

@INPROCEEDINGS{Cordts2016Cityscapes,
  title={The Cityscapes Dataset for Semantic Urban Scene Understanding},
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}
```

CUHK-SYSU:

```text
@inproceedings{xiaoli2017joint,
  title={Joint Detection and Identification Feature Learning for Person Search},
  author={Xiao, Tong and Li, Shuang and Wang, Bochao and Lin, Liang and Wang, Xiaogang},
  booktitle={CVPR},
  year={2017}
}
```

PRW:

```text
@inproceedings{zheng2017person,
  title={Person re-identification in the wild},
  author={Zheng, Liang and Zhang, Hengheng and Sun, Shaoyan and Chandraker, Manmohan and Yang, Yi and Tian, Qi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1367--1376},
  year={2017}
}
```

ETHZ:

```text
@InProceedings{eth_biwi_00534,
  author = {A. Ess and B. Leibe and K. Schindler and and L. van Gool},
  title = {A Mobile Vision System for Robust Multi-Person Tracking},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR'08)},
  year = {2008},
  month = {June},
  publisher = {IEEE Press},
}
```

MOT-16&17:

```text
@article{milan2016mot16,
  title={MOT16: A benchmark for multi-object tracking},
  author={Milan, Anton and Leal-Taix{\'e}, Laura and Reid, Ian and Roth, Stefan and Schindler, Konrad},
  journal={arXiv preprint arXiv:1603.00831},
  year={2016}
}
```
