# Builtin Configurations(DO NOT CHANGE THESE CONFIGURATIONS unless you know exactly what you are doing)
enable_modelarts: False
# Url for modelarts
data_url: ""
train_url: ""
checkpoint_url: ""
# Path for local
data_path: "/cache/data"
output_path: "/cache/train"
load_path: "/cache/checkpoint_path"
device_target: "GPU"
gpu_distribute_training: 1


# ==============================================================================
# options
dataset_name: "celeba"
name: "celeba"
resume_load_scale: -1
batch_size: 16
batch_size_list: [ 16, 16, 16, 16, 16, 8]
train_data_path: "/path/to/dataset/images"
resume_check_d: "chekpoint_from_huawei/D_12000.ckpt"
resume_check_g: "chekpoint_from_huawei/AvG_12000.ckpt"
ckpt_save_dir: "./checkpoint"
eval_img_save_dir: "./eval_img"
model_save_step: 1000
save_ckpt_from_device_with_id: 0
#network
scales: [4, 8, 16, 32, 64, 128]
depth: [512, 512, 512, 512, 256, 128]
num_batch: [48000, 96000, 96000, 96000, 96000, 96000]
alpha_jumps: [0, 600, 600, 600, 600, 600]
alpha_size_jumps: [32, 32, 32, 32, 32, 32]
# optimizer and lr related
lr: 0.002
lr_list: []
# loss related
loss_scale_value: 12
scale_factor: 10
scale_window: 1000
# export option
ckpt_file: ""
file_name: "PGAN"
file_format: "MINDIR"

---

# Help description for each configuration
enable_modelarts: "Whether training on modelarts, default: False"
data_url: "Url for modelarts"
train_url: "Url for modelarts"
data_path: "The location of the input data."
output_path: "The location of the output file."
device_target: 'Target device type'
distributed_training: 'Use more then 1 device to train network. 1 - true, 0 - false. Used only for GPU'
enable_profiling: 'Whether enable profiling while training, default: False'

dataset_name: "the name of the dataset being used"  #unused
name: "the name of the dataset being used" #unused
resume_load_scale: "resume training from this scale, must be in [4, 8, 16, 32, 64, 128]"
batch_size: "batch size used for training unless the batch_size_list is specified"
batch_size_list: "if not empty, specifies the bach size for each input scale. 
                 Must have the same length as the scales list"
train_data_path: "path to folder with images for network training"
resume_check_d: "discriminator checkpoint path"
resume_check_g: "generator checkpoint path"
ckpt_save_dir: "folder to save checkpoints"
eval_img_save_dir: "folder to save validation images"
model_save_step: "save the model every time through this number of steps"
save_ckpt_from_device_with_id: "save checkpoint from device with this id"
#network
scales: "generated image sizes"
depth: "input channel numbers for convolution in each layer"
num_batch: "Number of iterations for each image scale"
alpha_jumps: "do not change this parameter unless you know exactly what you are doing"
alpha_size_jumps: "do not change this parameter unless you know exactly what you are doing"
# optimizer and lr related
lr: "learning rate used for training unless the lr_list is specified"
lr_list: "if not empty, specifies the learning rate for each input scale. Must have the same length as the scales list"
# loss related
loss_scale_value: "parameter for model optimizer"
scale_factor: "parameter for model optimizer"
scale_window: "parameter for model optimizer"
# export option
ckpt_file: "" #unused
file_name: "file name for model export"
file_format: "choices in ['AIR', 'ONNX', 'MINDIR']"
