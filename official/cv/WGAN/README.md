# Contents

<!-- TOC -->

- [Contents](#contents)
- [WGAN Description](#wgan-description)
- [Model Architecture](#model-architecture)
- [Dataset](#dataset)
- [Environment Requirements](#environment-requirements)
- [Quick Start](#quick-start)
- [Script Description](#script-description)
    - [Script and Sample Code](#script-and-sample-code)
    - [Script Parameters](#script-parameters)
    - [Training Process](#training-process)
        - [Standalone Training](#standalone-training)
    - [Inference Process](#inference-process)
        - [Inference](#inference)
        - [ONNX Evaluation](#onnx-evaluation)
    - [Inference Process on Ascend 310 AI Processor](#inference-process-on-ascend-310-ai-processor)
        - [[Export MindIR]](#export-mindir)
        - [Inference on Ascend 310 Processor](#inference-on-ascend-310-processor)
        - [Result](#result)
- [Model Description](#model-description)
    - [Performance](#performance)
        - [Training Performance](#training-performance)
            - [1. Use the generator structure of the standard DCGAN.](#1-use-the-generator-structure-of-the-standard-dcgan)
            - [2. Use the generator structure of the standard DCGAN without BatchNorm.](#2-use-the-generator-structure-of-the-standard-dcgan-without-batchnorm)
        - [Inference Performance](#inference-performance)
            - [Inference](#inference-1)
- [Random Seed Description](#random-seed-description)
- [ModelZoo Home Page](#modelzoo-home-page)

<!-- /TOC -->

# WGAN Description

Wasserstein GAN, also named WGAN, is a generative adversarial network (GAN) based on Wasserstein distance, including a generator network and a discriminator network. It improves the original GAN algorithm process and the stability of GAN training, ensuring the diversity of generated samples. During training, values such as cross entropy and accuracy are used to indicate the training process. That is, a smaller **-loss_D** value indicates better GAN training and higher quality of an image generated by the generator.

[Payper](https://arxiv.org/abs/1701.07875): Martin Arjovsky, Soumith Chintala, Léon Bottou. "Wasserstein GAN" *In International Conference on Machine Learning (ICML 2017).

# Model Architecture

The WGAN network consists of two parts: generator network and discriminator network. The discriminator network adopts the architecture of Deep Convolutional Generative Adversarial Network (DCGAN), that is, 2D convolution with multiple layers. The generator network uses the DCGAN convolutional generator structure or the DCGAN convolutional generator structure without BatchNorm. The input data includes the real image data and noise data. The real image is resized to 64 x 64, and the noise data is generated randomly.

# Dataset

[LSUN-Bedrooms](<http://dl.yf.io/lsun/scenes/bedroom_train_lmdb.zip>)

- Dataset size: 42.8 GB
    - Training set: 42.8 GB, 3,033,044 images
    - Note: For the GAN, the test set is not required because the noise data is input to generate images during inference.
- Data format: The original data format is LMDB. You need to use the format conversion script on the LSUN official website to convert the format, export all images and place the Bedrooms images in the same folder.
    - Note: The address of the data format conversion script on the LSUN dataset official website is (<https://github.com/fyu/lsun>).

# Environment Requirements

- Hardware
    - Set up the hardware environment with Ascend AI Processors.
- Framework
    - [MindSpore](https://www.mindspore.cn/install/en)
- For more information, please check the following resources:
    - [MindSpore Tutorials](https://www.mindspore.cn/tutorials/en/master/index.html)
    - [MindSpore Python API](https://www.mindspore.cn/docs/en/master/index.html)

# Quick Start

After installing MindSpore from the official website, you can perform the following steps for training and evaluation:

- Running in the Ascend AI Processor Environment

  ```python
  # Run the standalone training (including the following two cases):
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] [NOBN]

  # 1. Use the generator structure of the standard DCGAN.
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] False

  # 2. Use the generator structure of the standard DCGAN without BatchNorm.
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] True


  # Evaluation
  bash run_eval.sh [DEVICE_ID] [CONFIG_PATH] [CKPT_FILE_PATH] [OUTPUT_DIR] [NIMAGES]
  ```

# Script Description

## Script and Sample Code

```bash
├── model_zoo
    ├── README.md                          // Description of all models
    ├── WGAN
        ├── README.md                    // WGAN description
        ├── scripts
        │   ├── run_train.sh          // Shell script for standalone training on the Ascend AI Processors
        │   ├──run_eval.sh              // Shell script for evaluation on Ascend AI Processors
        │   ├── run_eval_onnx.sh         // Shell script for ONNX evaluation
        ├── src
        │   ├── dataset.py             // Create a dataset and preprocess data.
        │   ├── dcgan_model.py            // WGAN architecture and standard DCGAN architecture
        │   ├── dcgannobn_model.py            // WGAN architecture, DCGAN architecture without BatchNorm
        │   ├── args.py               // Parameter configuration file
        │   ├── cell.py               // Model single-step training file
        ├── train.py              // Training script
        ├── eval.py               // Evaluation script
        ├── eval_onnx.py            // ONNX evaluation script
        ├── export.py               // Export the checkpoint file to MindIR
```

## Script Parameters

You can configure training parameters, evaluation parameters, and model export parameters in **args.py**.

  ```python
  # common_config
  'device_target': 'Ascend' # Running device
  'device_id': 0, # ID of the device used for training or evaluating the dataset

  # train_config
  'dataset': 'lsun', # Dataset name
  'dataroot': None, # Dataset path, which is mandatory and cannot be empty.
  'workers': 8, # Number of data loading threads
  'batchSize': 64, # Batch size
  'imageSize': 64, # Image size
  'nc': 3, # Number of channels for transferring images
  'nz': 100, # Size of the initial noise vector
  'ndf': 64, # Number of basic features of the discriminator network
  'ngf': 64, # Number of basic features of the generator network
  'niter': 25, # Number of epochs for network training
  'lrD': 0.00005, # Initial learning rate of the discriminator
  'lrG': 0.00005, # Initial learning rate of the generator
  'netG': '', # CKPT file path for the generator that resumes training
  'netD': '', # CKPT file path for the discriminator that resumes training
  'clamp_lower': -0.01, # Set the optimizer parameter to the lower limit of a range.
  'clamp_upper': 0.01, # Set the optimizer parameter to the upper limit of a range.
  'Diters': 5, # Number of the discriminator needs to be trained for each training of the generator
  'noBN': False, # Specifies whether to use BatchNorm in the convolution generator network. The default value is False.
  'n_extra_layers': 0, # Number of extra layers on the generator and discriminator networks. The default value is 0.
  'experiment': None, # Path for saving the model and generating the image. If this parameter is not specified, the default path is used.
  'adam': False, # Specifies whether to use the Adam optimizer. By default, the RMSprop optimizer is used instead of Adam optimizer.

  # eval_config
  'config': None, # Path for the .json configuration file of the generator generated during training, which must be specified.
  'ckpt_file': None, # Path for the generator weight file .ckpt saved during training, which must be specified.
  'output_dir': None, # Output path for the generated image, which must be specified.
  'nimages': 1, # Number of generated images. The default value is 1.

  # export_config
  'config': None, # Path for the .json configuration file of the generator generated during training, which must be specified.
  'ckpt_file': None, # Path for the generator weight file .ckpt saved during training, which must be specified.
  'file_name': 'WGAN', # Prefix of the output file name. The default value is WGAN.
  'file_format': 'AIR', # Model output format. The value can be AIR, ONNX, or MINDIR. The default value is AIR.
  'nimages': 1, # Number of generated images. The default value is 1.

  ```

For details about configuration, see the `args.py`.

## Training Process

### Standalone Training

- Running in the Ascend AI Processor Environment

  ```bash
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] [NOBN]
  ```

  1. Use the generator structure of the standard DCGAN.

  ```bash
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] False
  ```

  2. Use the generator structure of the standard DCGAN without BatchNorm.

  ```bash
  bash run_train.sh [DATASET] [DATAROOT] [DEVICE_ID] True
  ```

  The preceding Python command is executed in the backend. You can view the result in the **train.log** file.

  After the training is complete, you can find the generated images, checkpoint files, and .json files in the storage folder (./samples by default). The following methods are used to achieve the loss value:

  ```bash
  [0/25][2300/47391][23] Loss_D: -1.555344 Loss_G: 0.761238
  [0/25][2400/47391][24] Loss_D: -1.557617 Loss_G: 0.762344
  ...
  ```

## Inference Process

### Inference

- Evaluation on Ascend AI Processors

  Before running the following command, check the checkpoint and JSON file path used for inference and set the path for the output images.

  ```bash
  bash run_eval.sh [DEVICE_ID] [CONFIG_PATH] [CKPT_FILE_PATH] [OUTPUT_DIR] [NIMAGES]
  ```

  The preceding Python command runs in the backend. You can view the log information in the **eval/eval.log** file and view the generated images in the output image path.

### ONNX Evaluation

- Export your model to ONNX:

  ```bash
  python export.py --ckpt_file /path/to/wgan_generator.ckpt --file_name /path/to/wgan_generator --file_format ONNX --config generator_config.json --nimages 1
  ```

- Run ONNX evaluation from wgan directory:

  ```bash
  bash scripts/run_eval_onnx.sh <ONNX_MODEL_PATH> [DEVICE_TARGET] [N_IMAGES] [OUTPUT_DIR] [CONFIG]
  ```

  Resulting png files will be saved in the output directory.

## Inference Process on Ascend 310 AI Processor

**Set environment variables before inference by referring to [MindSpore C++ Inference Deployment Guide](https://gitee.com/mindspore/models/blob/master/utils/cpp_infer/README.md).**

### [Export MindIR]

```shell
python export.py --ckpt_file [CKPT_PATH] --file_name [FILE_NAME] --file_format [FILE_FORMAT]
```

The **ckpt_file** parameter is mandatory.
The value of parameter `file_format` must be selected from **AIR**, **ONNX**, **MINDIR**.

### Inference on Ascend 310 Processor

Before inference, the MindIR file must be exported through the `export.py` script. The following shows how to infer the MindIR model.

```shell
# Inference on Ascend 310 AI Processor
bash run_infer_310.sh [MINDIR_PATH] [CONFIG_PATH] [NEED_PREPROCESS] [NIMAGES] [DEVICE_ID]
```

- `NEED_PREPROCESS` specifies whether data needs to be preprocessed in binary format. The value can be **y** or **n**.
- `DEVICE_ID` is optional. The default value is **0**.

### Result

During the running of the preceding command, you can view the log information in the **infer.log** file and view the generated image in the path of the output image. By default, the image is stored in the **infer_output** directory in the current path.

# Model Description

## Performance

### Training Performance

#### 1. Use the generator structure of the standard DCGAN

| Parameter                | Ascend                                                     |
| -------------------------- | ----------------------------------------------------------- |
| Resources                  | Ascend 910 AI Processor; 2.60 GHz CPU with 192 cores; 755 GB memory            |
| Upload date             | 2021-05-14                                 |
| MindSpore version         | 1.2.0                                                 |
| Dataset                   | LSUN-Bedrooms                                                    |
| Training parameters       | max_epoch=25, batch_size=64, lr_init=0.00005              |
| Optimizer                 | RMSProp                                                    |
| Loss function             | Customized loss function                                      |
| Output                   | Generated images                                                |
| Speed                     | Single device: 190 ms/step                         |
| Total duration                | Single device: 12 hours and 10 minutes                      |
| Parameters (M)            | 6.57                                                        |
| Finetuned checkpoint| 13.98 MB (.ckpt file)                                        |
| Inference model       | 14.00 MB (.mindir file)                    |
| Script                   | [WGAN Script](https://gitee.com/mindspore/models/tree/r2.0/official/cv/WGAN)|

The following sample shows the generated image.

![GenSample1](imgs/WGAN_1.png "Image sample generated in the first case")

#### 2. Use the generator structure of the standard DCGAN without BatchNorm

| Parameter                | Ascend                                                     |
| -------------------------- | ----------------------------------------------------------- |
| Resources                  | Ascend 910 AI Processor; 2.60 GHz CPU with 192 cores; 755 GB memory            |
| Upload date             | 2021-05-14                                 |
| MindSpore version         | 1.2.0                                                 |
| Dataset                   | LSUN-Bedrooms                                                    |
| Training parameters       | max_epoch=25, batch_size=64, lr_init=0.00005              |
| Optimizer                 | RMSProp                                                    |
| Loss function             | Customized loss function                                      |
| Output                   | Generated images                                                |
| Speed                     | Single device: 180 ms/step                         |
| Total duration                | Single device: 11 hours and 40 minutes                      |
| Parameters (M)            | 6.45                                                        |
| Finetuned checkpoint| 13.98 MB (.ckpt file)                                        |
| Inference model       | 14.00 MB (.mindir file)                    |
| Script                   | [WGAN Script](https://gitee.com/mindspore/models/tree/r2.0/official/cv/WGAN)|

The following sample shows the generated image.

![GenSample2](imgs/WGAN_2.png "Image sample generated in the second case")

### Inference Performance

#### Inference

| Parameter         | Ascend                     |
| ------------------- | --------------------------- |
| Resources           | Ascend 910 AI Processor                 |
| Upload date      | 2021-05-14 |
| MindSpore version  | 1.2.0                 |
| Dataset            | LSUN-Bedrooms     |
| batch_size          | 1                         |
| Output            | Generated images                |

# Random Seed Description

We set a random seed in **train.py**.

# ModelZoo Home Page

 For details, please go to the [official website](https://gitee.com/mindspore/models).
